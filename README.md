# MonitorAI - LLM Monitoring Stack - Green Forest Theme

Monitoring stack Ä‘á»ƒ theo dÃµi cÃ¡c LLM (Large Language Model) processes Ä‘ang cháº¡y trÃªn mÃ¡y vá»›i Grafana, Prometheus vÃ  dashboard Green Forest theme.

## ğŸ“‹ MÃ´ táº£

Há»‡ thá»‘ng tá»± Ä‘á»™ng phÃ¡t hiá»‡n vÃ  theo dÃµi cÃ¡c LLM processes (transformers, llama.cpp, vLLM, TensorRT, ONNX, PyTorch, TensorFlow) vá»›i metrics:
- CPU vÃ  Memory usage per process
- GPU utilization vÃ  VRAM usage per process (há»— trá»£ Windows qua file JSON)
- Process count theo framework vÃ  model name
- Logs aggregation qua Loki
- Tá»± Ä‘á»™ng clear metrics cho processes Ä‘Ã£ dá»«ng

## ğŸ”„ Pipeline Há»‡ Thá»‘ng

Há»‡ thá»‘ng monitoring hoáº¡t Ä‘á»™ng theo 3 pipeline chÃ­nh:

### 1. Metrics Pipeline (CPU, Memory, GPU)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Processes â”‚
â”‚  (Python apps) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–º CPU/Memory metrics â”€â”€â”
         â”‚                        â”‚
         â””â”€â–º GPU info (JSON) â”€â”€â”€â”€â”€â”¤
            logs/gpu-info-*.json  â”‚
                                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      LLM Monitor (9101)         â”‚
         â”‚  - Detect LLM processes         â”‚
         â”‚  - Read GPU info files          â”‚
         â”‚  - Collect CPU/Memory/GPU       â”‚
         â”‚  - Expose Prometheus metrics    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   GPU Exporter (9100)          â”‚
         â”‚  - Query nvidia-smi             â”‚
         â”‚  - Collect overall GPU metrics  â”‚
         â”‚  - Expose Prometheus metrics    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      Prometheus (9090)          â”‚
         â”‚  - Scrape metrics every 15s     â”‚
         â”‚  - Store time-series data       â”‚
         â”‚  - Retention: 200 hours         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      Grafana (3000)             â”‚
         â”‚  - Query Prometheus via PromQL  â”‚
         â”‚  - Visualize in dashboards      â”‚
         â”‚  - Green Forest theme           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Chi tiáº¿t:**
- **LLM Processes** cháº¡y vÃ  ghi GPU memory vÃ o `logs/gpu-info-{PID}.json` (má»—i 10 giÃ¢y)
- **LLM Monitor** (port 9101):
  - QuÃ©t táº¥t cáº£ processes má»—i 10 giÃ¢y
  - Äá»c GPU info files tá»« `logs/` directory
  - Thu tháº­p CPU, Memory, GPU metrics per process
  - Expose metrics qua Prometheus client library
- **GPU Exporter** (port 9100):
  - Query `nvidia-smi` má»—i 15 giÃ¢y
  - Thu tháº­p overall GPU metrics (utilization, memory, temperature, power)
  - Expose metrics qua Prometheus client library
- **Prometheus** (port 9090):
  - Scrape LLM Monitor vÃ  GPU Exporter má»—i 15 giÃ¢y
  - LÆ°u trá»¯ time-series data vá»›i retention 200 giá»
  - Cung cáº¥p PromQL Ä‘á»ƒ query metrics
- **Grafana** (port 3000):
  - Káº¿t ná»‘i Ä‘áº¿n Prometheus qua datasource
  - Hiá»ƒn thá»‹ metrics trong dashboard vá»›i Green Forest theme
  - Auto-refresh má»—i 15 giÃ¢y

### 2. Logs Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Processes â”‚
â”‚  (Python apps)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Write logs
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ logs/llm-model. â”‚
â”‚      log        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Read logs
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Promtail      â”‚
â”‚  (Log Shipper)  â”‚
â”‚  - Tail log fileâ”‚
â”‚  - Parse & labelâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Push logs
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Loki (3100)   â”‚
â”‚  - Store logs   â”‚
â”‚  - Index by     â”‚
â”‚    labels       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Query logs
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Grafana       â”‚
â”‚  - Logs panel   â”‚
â”‚  - LogQL queriesâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Chi tiáº¿t:**
- **LLM Processes** ghi logs vÃ o `logs/llm-model.log` (format: timestamp, level, message)
- **Promtail** (Docker container):
  - Äá»c log file tá»« `logs/` directory (mounted volume)
  - Parse vÃ  label logs vá»›i `job=llm-model`
  - Push logs Ä‘áº¿n Loki qua HTTP API
- **Loki** (port 3100):
  - Nháº­n vÃ  lÆ°u trá»¯ logs
  - Index logs theo labels Ä‘á»ƒ query nhanh
  - Cung cáº¥p LogQL Ä‘á»ƒ query logs
- **Grafana**:
  - Káº¿t ná»‘i Ä‘áº¿n Loki qua datasource
  - Hiá»ƒn thá»‹ logs trong Logs panel
  - Há»— trá»£ LogQL queries vÃ  filtering

### 3. Tracing Pipeline (Optional - Future)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Applications   â”‚
â”‚  (OpenTelemetry)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Send traces
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tempo (3200)    â”‚
â”‚  - Store traces  â”‚
â”‚  - OTLP protocol â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Query traces
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Grafana       â”‚
â”‚  - Trace view   â”‚
â”‚  - Flame graphs â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Chi tiáº¿t:**
- **Tempo** (port 3200) sáºµn sÃ ng nháº­n traces qua OTLP (gRPC port 4317, HTTP port 4318)
- Hiá»‡n táº¡i chÆ°a cÃ³ application gá»­i traces, nhÆ°ng infrastructure Ä‘Ã£ sáºµn sÃ ng
- CÃ³ thá»ƒ tÃ­ch há»£p OpenTelemetry SDK vÃ o LLM processes Ä‘á»ƒ gá»­i traces

### Data Flow Summary

| Component | Input | Output | Frequency |
|-----------|-------|--------|-----------|
| LLM Processes | - | GPU info JSON, Logs | 10s (inference) |
| LLM Monitor | Processes, GPU JSON | Prometheus metrics | 10s |
| GPU Exporter | nvidia-smi | Prometheus metrics | 15s |
| Promtail | Log files | Loki logs | Real-time |
| Prometheus | Metrics endpoints | Time-series DB | 15s scrape |
| Loki | Promtail logs | Log storage | Real-time |
| Grafana | Prometheus, Loki | Dashboard | 15s refresh |

## ğŸš€ HÆ°á»›ng dáº«n cháº¡y

### YÃªu cáº§u
- Docker Desktop Ä‘ang cháº¡y
- Conda environment `Grafotel` vá»›i Python 3.11+
- NVIDIA GPU vá»›i nvidia-smi (optional, cho GPU metrics)
- PyTorch vá»›i CUDA support (cho GPU monitoring chÃ­nh xÃ¡c)

### BÆ°á»›c 1: Start Táº¥t Cáº£ Services

```powershell
.\start-all.ps1
```

Script nÃ y sáº½ tá»± Ä‘á»™ng khá»Ÿi Ä‘á»™ng:
- Docker services: Grafana (3000), Prometheus (9090), Loki (3100), Tempo (3200)
- LLM Monitor (port 9101) - cháº¡y background
- GPU Exporter (port 9100) - cháº¡y background (náº¿u cÃ³ NVIDIA GPU)

### BÆ°á»›c 2: Cháº¡y LLM Model

**Option 1: Cháº¡y model vá»›i GPU (khuyáº¿n nghá»‹)**

Má»Ÿ terminal má»›i, kÃ­ch hoáº¡t conda environment vÃ  cháº¡y:

```powershell
conda activate Grafotel
python run-llm-model-gpu.py
```

Script nÃ y sáº½:
- Tá»± Ä‘á»™ng detect GPU vÃ  load model lÃªn GPU
- Expose GPU memory usage qua file JSON (`logs/gpu-info-{PID}.json`)
- LLM Monitor sáº½ Ä‘á»c file nÃ y Ä‘á»ƒ láº¥y GPU metrics chÃ­nh xÃ¡c

**Option 2: Cháº¡y model CPU hoáº·c GPU (tá»± Ä‘á»™ng detect)**

```powershell
conda activate Grafotel
python run-llm-model.py --model-name microsoft/DialoGPT-small
```

LLM Monitor sáº½ tá»± Ä‘á»™ng detect model vÃ  collect metrics (CPU, Memory, GPU náº¿u cÃ³).

### BÆ°á»›c 3: Xem Dashboard

- Truy cáº­p: http://localhost:3000
- Login: `admin` / `admin`
- Dashboard: **Dashboards** â†’ **LLM Monitoring â€“ Green Forest Dashboard**

## ğŸ›‘ Dá»«ng services

```powershell
.\stop-all.ps1
```

Dá»«ng táº¥t cáº£: Docker services, LLM Monitor, GPU Exporter

## ğŸ“‹ Xem Logs

```powershell
.\start-all.ps1 -ViewLogs
```

Logs Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c `logs/`:
- `logs/llm-model.log` - Logs tá»« LLM models
- `logs/gpu-info-{PID}.json` - GPU memory info tá»« processes (tá»± Ä‘á»™ng táº¡o)

## ğŸ§¹ Reset Docker (xÃ³a táº¥t cáº£ data)

```powershell
.\stop-all.ps1
docker-compose down -v
```

## ğŸ“ Cáº¥u trÃºc project

```
MonitorAI/
â”œâ”€â”€ config/                  # Config files cho Grafana, Prometheus, Loki, Tempo, Promtail
â”‚   â”œâ”€â”€ grafana/            # Grafana datasources vÃ  dashboard provider
â”‚   â”œâ”€â”€ prometheus/         # Prometheus config
â”‚   â”œâ”€â”€ loki/               # Loki config
â”‚   â”œâ”€â”€ tempo/              # Tempo config
â”‚   â””â”€â”€ promtail/           # Promtail config (log shipping)
â”œâ”€â”€ dashboards/              # LLM monitoring dashboard
â”‚   â””â”€â”€ llm-monitoring-green-forest.json
â”œâ”€â”€ llm-monitor/             # LLM Process Monitor
â”‚   â”œâ”€â”€ llm_monitor.py      # Main monitor script
â”‚   â””â”€â”€ requirements.txt    # Python dependencies (bao gá»“m nvidia-ml-py)
â”œâ”€â”€ gpu-exporter/            # GPU Metrics Exporter
â”‚   â”œâ”€â”€ gpu_exporter.py     # GPU metrics collector
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ logs/                    # Logs directory (tá»± Ä‘á»™ng táº¡o)
â”‚   â”œâ”€â”€ llm-model.log       # LLM model logs
â”‚   â””â”€â”€ gpu-info-*.json     # GPU info files (tá»± Ä‘á»™ng táº¡o bá»Ÿi processes)
â”œâ”€â”€ docker-compose.yml       # Docker services
â”œâ”€â”€ start-all.ps1            # Start all services (Docker + LLM Monitor + GPU Exporter)
â”œâ”€â”€ stop-all.ps1             # Stop all services
â”œâ”€â”€ run-llm-model.py         # Example LLM model script (CPU/GPU auto-detect)
â””â”€â”€ run-llm-model-gpu.py    # GPU-optimized LLM model script
```

## ğŸ” LLM Detection

Tá»± Ä‘á»™ng phÃ¡t hiá»‡n cÃ¡c frameworks:
- Hugging Face Transformers
- llama.cpp
- vLLM
- TensorRT
- ONNX Runtime
- PyTorch
- TensorFlow

Model names Ä‘Æ°á»£c tá»± Ä‘á»™ng extract tá»« command line hoáº·c file paths.

## ğŸ¯ GPU Monitoring (Windows)

TrÃªn Windows, GPU monitoring sá»­ dá»¥ng cÆ¡ cháº¿ **file-based exposure** Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c:

1. **Process tá»± expose GPU memory:**
   - `run-llm-model-gpu.py` vÃ  `run-llm-model.py` ghi GPU memory vÃ o `logs/gpu-info-{PID}.json`
   - File Ä‘Æ°á»£c cáº­p nháº­t má»—i láº§n inference (má»—i 10 giÃ¢y)
   - Chá»©a: `pid`, `gpu_memory_allocated_bytes`, `gpu_memory_reserved_bytes`, `gpu_utilization`, `gpu_index`, `timestamp`

2. **LLM Monitor Ä‘á»c file JSON:**
   - Äá»c táº¥t cáº£ file `logs/gpu-info-*.json` má»—i 10 giÃ¢y
   - Láº¥y GPU memory chÃ­nh xÃ¡c tá»« PyTorch (thay vÃ¬ dá»±a vÃ o `nvidia-smi` cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c trÃªn Windows)
   - Tá»± Ä‘á»™ng xÃ³a file náº¿u process khÃ´ng cÃ²n tá»“n táº¡i

3. **Metrics Ä‘Æ°á»£c expose:**
   - `llm_process_gpu_memory_bytes` - GPU memory usage per process
   - `llm_process_gpu_utilization` - GPU utilization per process
   - `nvidia_gpu_utilization` - Overall GPU utilization
   - `nvidia_gpu_memory_used_bytes` - Overall GPU memory used
   - `nvidia_gpu_memory_total_bytes` - Total GPU memory
   - `nvidia_gpu_temperature` - GPU temperature
   - `nvidia_gpu_power_usage` - GPU power usage

## ğŸ› Troubleshooting

**LLM Monitor khÃ´ng detect processes:**
- Kiá»ƒm tra cÃ³ Python processes Ä‘ang cháº¡y: `Get-Process python`
- Verify LLM Monitor Ä‘ang cháº¡y: `curl http://localhost:9101/metrics`
- Äáº£m báº£o Ä‘ang cháº¡y trong conda environment `Grafotel`

**Dashboard khÃ´ng cÃ³ data:**
- Äáº£m báº£o LLM Monitor Ä‘ang cháº¡y
- Kiá»ƒm tra time range (Last 15 minutes)
- Verify Prometheus cÃ³ metrics: http://localhost:9090/graph?g0.expr=llm_process_count
- Kiá»ƒm tra Prometheus targets: http://localhost:9090/targets

**GPU metrics khÃ´ng hiá»‡n hoáº·c hiá»‡n 0.0:**
- Kiá»ƒm tra GPU Exporter: `curl http://localhost:9100/metrics`
- Verify nvidia-smi hoáº¡t Ä‘á»™ng: `nvidia-smi`
- Kiá»ƒm tra PyTorch cÃ³ CUDA: `python -c "import torch; print(torch.cuda.is_available())"`
- Kiá»ƒm tra file GPU info: `Get-ChildItem logs\gpu-info-*.json`
- Äáº£m báº£o model Ä‘ang cháº¡y vá»›i GPU: sá»­ dá»¥ng `run-llm-model-gpu.py`
- Restart LLM Monitor sau khi model báº¯t Ä‘áº§u cháº¡y

**Process Ä‘Ã£ dá»«ng nhÆ°ng váº«n hiá»‡n trÃªn dashboard:**
- LLM Monitor tá»± Ä‘á»™ng clear metrics sau 10 giÃ¢y
- Náº¿u váº«n hiá»‡n, restart LLM Monitor: `.\stop-all.ps1` rá»“i `.\start-all.ps1`

**Logs khÃ´ng hiá»‡n trong Grafana:**
- Kiá»ƒm tra Promtail Ä‘ang cháº¡y: `docker ps | Select-String promtail`
- Verify logs file tá»“n táº¡i: `Test-Path logs\llm-model.log`
- Kiá»ƒm tra Loki: http://localhost:3100/ready

## ğŸ“ License

MIT License
